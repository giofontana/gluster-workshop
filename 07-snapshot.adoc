1. Enable Snapshots
In this lab, you enable snapshots and use the glustervol distributed-replicated volume for testing snapshots.

Enable the Red Hat Gluster Storage volume to use non-privileged ports for the snapshot bricks:

# gluster volume set glustervol server.allow-insecure on
In a production environment, edit /etc/glusterfs/glusterd.vol on each storage node and add the following line: option rpc-auth-allow-insecure on.

Check the status of glustervol:

# gluster volume info glustervol
Your output should be similar to this showing the bricks that make up the volume:

glustervol info
Examine the state of the logical volume before the snapshot:


# lsblk
lsblk before
2. Create Snapshot
In this section, you create a snapshot called snap1 and use the no-timestamp option to avoid having a timestamp appended to the name of the snapshot.

On rhgs1, create a snapshot of glustervol called snap1:

# gluster snapshot create snap1 glustervol no-timestamp
snapshot create: success: Snap snap1 created successfully


List all of the snapshots for glustervol:

# gluster snapshot list glustervol
View detailed snapshot information for snap1:

# gluster snapshot info snap1
----
Snapshot                  : snap1
Snap UUID                 : 4da92aeb-673b-48cc-99a7-0ce70b8afc51
Created                   : 2020-03-10 23:22:38
Snap Volumes:

	Snap Volume Name          : ab869479f2fc42468146cf25d1bcce1d
	Origin Volume name        : glustervol
	Snaps taken for glustervol      : 1
	Snaps available for glustervol  : 255
	Status                    : Stopped
----

Note that the snapshot status is Stopped by default.

gluster snapshot activate snap1



Check the status of the snap1 snapshot:

# gluster snapshot status snap1
The sample output shows the different bricks that compose this snapshot volume:

snapshot status
Use lsblk again on rhgs1 to view the state of the volume after the snapshot:

# lsblk
Note that the thinly provisioned snapshot logical volume in this sample output is created and used as a brick for the snapshot volume:

lsblk after
3. Recover File From Deleted Directory
In this section, you recover a deleted directory using the snapshot of the original Red Hat Gluster Storage volume.

Activate the snapshot:

# gluster snapshot activate snap1
Note that it is in Stopped state by default.

On client1, list the contents of /mnt/glustervol and /mnt/glustervol/test2:

# ls /mnt/glustervol/
# ls /mnt/glustervol/test2
Remove the directory /mnt/glustervol/test2 and confirm that it does not exist:

# rm -rf /mnt/glustervol/test2
# ls /mnt/glustervol/test2
Mount the snap1 snapshot using the Red Hat Gluster Native protocol under /mnt/snap:

# mkdir /mnt/snap
# mount -t glusterfs rhgs1:/snaps/snap1/glustervol /mnt/snap/
Check whether you can access the deleted test2 directory and its contents under /mnt/snap/test:

# df -h
# ls /mnt/snap/test2/
The output from the preceding four steps should look similar to this:

mount snap1
On rhgs1, view the snapshot configuration information for the glustervol volume:

# gluster snapshot config glustervol

Snapshot System Configuration:
snap-max-hard-limit : 256
snap-max-soft-limit : 90%
auto-delete : disable
activate-on-create : disable

Snapshot Volume Configuration:

Volume : glustervol
snap-max-hard-limit : 256
Effective snap-max-hard-limit : 256
Effective snap-max-soft-limit : 230 (90%)
On client1, create a file in the glustervol volume:

[root@client1-GUID ~]# ls /mnt/glustervol/
[root@client1-GUID ~]# touch /mnt/glustervol/hello.world
[root@client1-GUID ~]# ls /mnt/glustervol/
hello.world
On client1, unmount the glustervol volume:

[root@client1-GUID ~]# umount /mnt/glustervol
On rhgs1, stop the glustervol volume:

[root@rhgs1 ~]# gluster volume stop glustervol
Stopping volume will make its data inaccessible. Do you want to continue? (y/n) y
volume stop: glustervol: success
On rhgs1, restore the snap1 snapshot and restart the mirrovol volume:

[root@rhgs1 ~]# gluster snapshot restore snap1
Restore operation will replace the original volume with the snapshotted volume. Do you still want to continue? (y/n) y
Snapshot restore: snap1: Snap restored successfully

[root@rhgs1 ~]# gluster volume start glustervol
volume start: glustervol: success
Outside the lab environment, after restoring snapshots you should perform a full heal by issuing the gluster volume heal glustervol full command to make sure the storage system is fully consistent.

You do not need to complete this in the lab environment.

On client1, mount the glustervol volume again and list its contents:

[root@client1-GUID ~]# mount -t glusterfs rhgs1:/glustervol /mnt/glustervol
[root@client1-GUID ~]# ls /mnt/glustervol/
[root@client1-GUID ~]# ls /mnt/glustervol/test2
[root@client1-GUID ~]# ls /mnt/glustervol/hello.world
Confirm that the glustervol volume with test2 directory and 100 files are there, but that the hello.world file is not.

Because the volume snapshot was created with these files on the volume, all changes to the volume were lost when the snapshot was restored.

4. Activate and Deactivate Snapshot
You can access only activated snapshots. All snapshots are in a Stopped state (deactivated) by default. Since each snapshot captures a Red Hat Gluster Storage volume, it consumes resources. To more efficiently manage your resources, deactivate unneeded snapshots and activate them only when needed.

To activate a snapshot, run the gluster snapshot activate <snapname> [force] command.

To deactivate a snapshot, run the gluster snapshot deactivate <snapname> [force] command.

5. Access Snapshot
On rhgs1 create a new snapshot:

[root@rhgs1 ~]# gluster snapshot create snap2 glustervol no-timestamp
snapshot create: success: Snap snap2 created successfully
Activate the snapshot so that you can access it:

[root@rhgs1 ~]#  gluster snapshot activate snap2
Snapshot activate: snap2: Snap activated successfully
On client1, create a new mount point and mount the special location of the snap2 snapshot:

[root@client1-GUID ~]# mkdir /mnt/snap2
[root@client1-GUID ~]# mount -t glusterfs rhgs1:/snaps/snap2/glustervol /mnt/snap2
On client1 create a new file in /mnt/glustervol:

[root@client1-GUID ~]# touch /mnt/glustervol/newfile1
Inspect the file you created in the glustervol volume:

[root@client1-GUID ~]# ls /mnt/snap2/
[root@client1-GUID ~]#
Note that it is not viewable in the snap2 snapshot.

6. Enable User-Serviceable Snapshots
On rhgs1, enable user-serviceable snapshots for the glustervol volume:

[root@rhgs1 ~]# gluster volume set glustervol features.uss enable
volume set: success
7. View and Retrieve Snapshot Using NFS/FUSE
Snapshots provide any user who has access to the volume with a read-only view of the volume. You can use these read-only views to recover files with different timestamps. Every directory of the mounted volume contains a .snaps directory where each snapshot of the volume is stored. Because the .snaps directory is a virtual directory, it does not appear in the lists generated when you use either the ls command or the ls -a option.

The .snaps directory holds every snapshot taken for that given volume as individual directories. Each snapshot entry contains the data of the particular directory at the point when the snapshot was taken.

On rhgs1, create and activate a new snapshot of glustervol called snap3:

[root@rhgs1 ~]# gluster snapshot create snap3 glustervol  no-timestamp description "My Third Snapshot"
snapshot create: success: Snap snap3 created successfully
[root@rhgs1 ~]# gluster snapshot activate snap3
Snapshot activate: snap3: Snap activated successfully
On client1, create a new file in /mnt/glustervol:

[root@client1-GUID ~]# touch /mnt/glustervol/newfile2
On client1, list the contents of /mnt/glustervol:

[root@client1-GUID .snaps]# ls /mnt/glustervol
newfile1  newfile2 test2
On client1, list contents of the user-serviceable snapshot directory /mnt/glustervol/.snaps:

[root@client1-GUID .snaps]# ls /mnt/glustervol/.snaps
snap2  snap3
On client1, list contents of the user-serviceable snapshot directory /mnt/glustervol/.snaps/snap2:

[root@client1-GUID .snaps]# ls /mnt/glustervol/.snaps/snap2
You should see that snap2 is empty because you created the snapshot before creating any files.

On client1, list contents of the user-serviceable snapshot directory /mnt/glustervol/.snaps/snap3:

[root@client1-GUID .snaps]# ls /mnt/glustervol/.snaps/snap3
newfile1 test2
You should see that snap3 contains one new file, which is included in the snapshot because the file existed when snap3 was created.